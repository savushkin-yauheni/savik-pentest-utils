import html
import operator
import os
import re
import urllib
from collections import OrderedDict
from typing import Dict, List, Optional, Set, Tuple, Union
from urllib.parse import parse_qs, quote, urlencode, urlparse, urlsplit, urlunsplit

import tldextract
import validators
from utils.collections_utils import flat_list_of_lists, is_collection, merge_dicts, remove_empties
from utils.io_utils import get_file_extension
from utils.string_utils import decode_utf8

MAX_URL_LENGTH = 200


HTTPS_PORT = 443
HTTPS_SCHEME = 'https'
HTTP_SCHEME = 'http'
HTTP_PORT = 80

def parse_url_params(url: str) -> Dict[str, List[str]]:
    return parse_qs(urlparse(url).query)


INVALID_CHARS_IN_PARAMS = ['/', '<', '>', ' ', '!', ';', ',', '"', "'", ')', '(', ':']


def parse_only_good_url_params(url: str) -> Dict[str, List[str]]:
    params = parse_url_params(url)

    def is_good_param(param_name: str) -> bool:
        for char in INVALID_CHARS_IN_PARAMS:
            if char in param_name:
                return False
        return True

    return {param: values for param, values in params.items() if is_good_param(param)}


def get_url_params(url) -> Set[str]:
    params = set(parse_url_params(url).keys())
    return {param.replace('?', '') for param in params}


def collect_params(urls: Set[str]):
    return set(flat_list_of_lists([list(get_url_params(url)) for url in urls]))


def get_url_without_params(url: str) -> str:
    return '{uri.scheme}://{uri.netloc}{uri.path}'.format(uri=urlparse(url))


def get_path_without_params(path: str) -> str:
    return '{uri.path}'.format(uri=urlparse(path))


def create_query_str(params_dict: Union[Dict[str, List[str]], Dict[str, str]], *,
                     disable_urlencode: bool = False) -> str:
    query_params = {
        param: values[0] if is_collection(values) and len(values) == 1 else values
        for param, values in params_dict.items()
    }
    if disable_urlencode:
        return '?' + '&'.join(f'{k}={v}' for k, v in query_params.items())
    return '?' + urlencode(
        OrderedDict(sorted(query_params.items(), key=lambda t: t[0])), quote_via=quote  # type: ignore
    ) if len(query_params.items()) else ''


def replace_params(url: str, params_dict: Union[Dict[str, List[str]], Dict[str, str]], *,
                   disable_urlencode=False) -> str:
    return get_url_without_params(url) + create_query_str(params_dict, disable_urlencode=disable_urlencode)


def replace_specific_params(url: str, params_dict: Union[Dict[str, List[str]], Dict[str, str]], *,
                            disable_urlencode=False) -> str:
    url_params = parse_url_params(url)
    merged_params = merge_dicts(url_params, params_dict)
    return get_url_without_params(url) + create_query_str(merged_params, disable_urlencode=disable_urlencode)


def replace_host_in_url(url: str, new_host: str):
    url_port = get_url_port(url)
    url_parts = list(urlsplit(url))
    url_parts[1] = new_host if url_port in DEFAULT_WEB_PORTS else f'{new_host}:{url_port}'
    new_url = urlunsplit(url_parts)
    return new_url


def replace_host_and_port_in_url(url: str, new_host_with_port: str):
    url_parts = list(urlsplit(url))
    url_parts[1] = new_host_with_port
    new_url = urlunsplit(url_parts)
    return new_url


def replace_scheme_in_url(url: str, new_scheme: str):
    url_parts = list(urlsplit(url))
    url_parts[0] = new_scheme
    new_url = urlunsplit(url_parts)
    return new_url


def get_top_n_params(urls: Set[str], n: int) -> Set[str]:
    counters: Dict[str, int] = {}
    for url in urls:
        url_params = get_url_params(url)
        for param in url_params:
            counters[param] = counters.setdefault(param, 0) + 1
    d_ascending = OrderedDict(sorted(counters.items(), key=operator.itemgetter(1), reverse=True))
    return {param[0] for param in list(d_ascending.items())[0:n]}


def combine_and_remove_duplicate_urls(urls: Union[List[str], Set[str]]) -> List[str]:
    combined_url_params: Dict[str, Dict[str, List[str]]] = {}
    for url in urls:
        if is_url(url):
            combined_url_params.setdefault(get_url_without_params(url), {}).update(parse_url_params(url))

    combined_urls = []
    for host, param_to_values in combined_url_params.items():
        params = sorted(param_to_values.keys())
        query = {param: param_to_values.setdefault(param, []) for param in params}
        combined_urls.append(replace_params(host, query))
    return combined_urls


def remove_params(urls: Set[str]) -> Set[str]:
    return {get_url_without_params(url) for url in urls}


def encode_url(url: str) -> str:
    uri = urlparse(url)
    query = f'?{uri.query}' if uri.query else ''
    return replace_params(f'{uri.scheme}://{uri.netloc}{encode_str(uri.path)}{query}', parse_url_params(url))


def encode_urls(urls: Set[str]) -> Set[str]:
    return {encode_url(url) for url in urls}


def encode_str(data: str, *, safe: str = None) -> str:
    return urllib.parse.quote(data) if safe is None else urllib.parse.quote(data, safe=safe)


def url_has_scheme(url: str) -> bool:
    if not is_url(url):
        return False
    return bool(get_url_scheme(url))


def get_url_scheme(url: str) -> str:
    return urlsplit(url).scheme


def get_url_port(url: str) -> int:
    if not url_has_scheme(url):
        raise Exception(f'{url} should have scheme')
    port = parse_port(url)
    if port:
        return port
    return HTTPS_PORT if is_https(url) else HTTP_PORT


def parse_port(url) -> Optional[int]:
    return urlparse(url).port


def remove_url_port(url: str):
    return f'{get_url_scheme(url)}://{get_fqdn(url)}'


def is_https(url: str) -> bool:
    return get_url_scheme(url) == HTTPS_SCHEME


def with_protocol(domain: str) -> str:
    return domain if url_has_scheme(domain) else append_https_protocol(domain)


def with_protocols(domain: Union[List[str], Set[str]]) -> Set[str]:
    return {with_protocol(url) for url in domain}


def append_https_protocol(domain: str) -> str:
    if url_has_scheme(domain):
        raise Exception(f'{domain} already has scheme')
    return f'{HTTPS_SCHEME}://' + domain


def append_http_protocol(domain: str) -> str:
    if url_has_scheme(domain):
        raise Exception(f'{domain} already has scheme')
    return 'http://' + domain


def append_port(web_host: str, port: int) -> str:
    return f'{web_host}:{port}'


def is_absolute_url(url: str) -> bool:
    return bool(urlparse(url).netloc)


def get_base_url(url: str) -> str:
    # if not is_url(url):
    #     raise ValueError(f"[get_base_url] it's not a url: {url}")
    parsed_uri = urlparse(url)
    return '{uri.scheme}://{uri.netloc}'.format(uri=parsed_uri)


def get_netloc(url: str) -> str:
    return urlparse(url).netloc


def remove_default_ports_and_domain_dot(url: str) -> str:
    netloc = get_netloc(url)
    if netloc.endswith('.'):
        netloc = netloc[:-1]
    if '.:' in netloc:
        netloc = netloc.replace('.:', ':')
    if ':' in netloc:
        if get_url_port(url) in [HTTP_PORT, HTTPS_PORT]:
            netloc = netloc[:netloc.index(':')]
    return replace_host_and_port_in_url(url, netloc)


def url_has_username(url: str) -> bool:
    return bool(urlparse(url).username)


def create_absolute_url(base_url: str, abs_or_relative_url: str) -> str:
    if is_absolute_url(abs_or_relative_url):
        return abs_or_relative_url
    base_domain = get_base_url(base_url)
    return urllib.parse.urljoin(base_domain, abs_or_relative_url)


def url_join_paths(*paths) -> str:
    return "/".join(map(lambda x: str(x).rstrip('/'), paths))


def is_url(url: str) -> bool:
    is_url_flag = validators.url(url)
    if is_url_flag is not True and ('--' in url or '_' in url or ':' in url):
        is_url_flag = custom_uri_validator(url)
    if is_url_flag is True:
        try:
            parse_port(url)
        except ValueError as ex:
            if 'Port out of range' in str(ex):
                return False

        scheme = get_url_scheme(url)
        if scheme not in [HTTPS_SCHEME, HTTP_SCHEME]:
            return False
    return is_url_flag is True


url_regex = re.compile(
    r'^(?:http|ftp)s?://'  # http:// or https://
    r'(?:(?:[A-Z0-9](?:[A-Z0-9-_]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|'  # domain...
    r'localhost|'  # localhost...
    r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
    r'(?::\d+)?'  # optional port
    r'(?:/?|[/?]\S+)$', re.IGNORECASE)


def custom_uri_validator(url: str) -> bool:
    return re.match(url_regex, url) is not None


def exclude_none_urls(urls: Union[List[str], Set[str]]) -> Set[str]:
    return {url for url in urls if is_url(url)}


def is_domain_name(domain: str) -> bool:
    return validators.domain(domain) == True


def get_web_domains_from_urls(urls: Union[List[str], Set[str]]) -> List[str]:
    return list({get_base_url(url) for url in urls})


def get_fqdn(url: str) -> str:
    return tldextract.extract(url).fqdn


def get_domain(url: str) -> str:
    return tldextract.extract(url).domain


def get_ipv4(url: str) -> str:
    return tldextract.extract(url).ipv4


def get_registered_domain(url: str) -> str:
    return tldextract.extract(url).registered_domain


def filter_urls_by_reg_domain(urls: Union[List[str], Set[str]], reg_domain_or_url: str) -> Set[str]:
    return {url for url in urls if get_registered_domain(url) == get_registered_domain(reg_domain_or_url)}


def encode_urls_to_ascii(urls: Union[List[str], Set[str]]) -> Set[str]:
    return {decode_utf8(url.encode('ascii', 'ignore')) for url in urls}


def url_has_subdomain(url: str) -> bool:
    return bool(get_url_subdomain(url))


def get_url_subdomain(url: str) -> str:
    return tldextract.extract(url).subdomain


def get_file_name_from_url(url: str) -> str:
    return os.path.basename(urlparse(with_protocol(url)).path)


def get_url_extension(url: str) -> str:
    return get_file_extension(get_file_name_from_url(url))


def get_domains_from_urls(urls: Union[List[str], Set[str]]) -> List[str]:
    return list({get_fqdn(url) for url in urls})


def normalize_url(url: str, src: str) -> str:
    url = url.rstrip('/')
    src = src.strip()
    src = src.rstrip('/')

    # Protocol relative URL
    if src.startswith('//'):
        return f'{get_url_scheme(url)}:{src}'

    # Relative URL with /
    if src.startswith('/'):
        return f'{get_base_url(url)}{src}'

    # Relative URL with ?
    if src.startswith('?'):
        return f'{url}/{src}'

    # Relative URL with ./
    if src.startswith('./'):
        return f'{url}{src[1:]}'

    # Absolute URL
    if src.startswith('https://') or src.startswith('http://'):
        return src

    # Else let's hope it is relative URL
    return f'{url}/{src}'


def url_has_domain(url: str, domain: str):
    return tldextract.extract(url).fqdn == tldextract.extract(domain).fqdn


def unquote_url(url: str) -> str:
    decoded_url = temp = url
    attempt = 0
    while True:
        if attempt > 150:
            raise Exception(f'check url {url}')
        decoded_url = urllib.parse.unquote(decoded_url)
        decoded_url = html.unescape(decoded_url)
        if decoded_url == temp:
            return decoded_url
        attempt += 1
        temp = decoded_url


def unquote_urls(urls: Set[str]) -> Set[str]:
    result = set()
    for url in urls:
        try:
            result.add(unquote_url(url))
        except:
            continue
    return result


def remove_amp_urls(urls: Set[str]) -> Set[str]:
    return {url for url in urls if url.count('%3Bamp') < 3 and url.count(';amp') < 3}


URL_REGEX = 'https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9(@:%_\\+.~#?&\\/=]*)'


def extract_all_urls(text: str) -> Set[str]:
    return {temp for temp in set(re.findall(URL_REGEX, text)) if is_url(temp)}


DOMAIN_REGEX = r'(?:[a-zA-Z0-9](?:[a-zA-Z0-9\-]{,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,10}'


def extract_all_domain_names(text: str) -> Set[str]:
    return {temp for temp in set(re.findall(DOMAIN_REGEX, text)) if is_domain_name(temp)}


def get_url_path(url: str) -> str:
    return urlparse(url).path


def strip_scheme(url: str) -> str:
    parsed = urlparse(url)
    scheme = "%s://" % parsed.scheme
    return parsed.geturl().replace(scheme, '', 1)


def get_url_path_with_params(url: str) -> str:
    path = get_url_path(url)
    if not path:
        return path
    url_without_scheme = strip_scheme(url)
    return url_without_scheme[url_without_scheme.index('/'):]


def get_url_path_parts(url: str) -> List[str]:
    path = get_url_path(url)
    if not path or path == '/':
        return []
    parts = remove_empties(path.split('/'))
    return parts


def are_web_hosts_equal(web_host_one: str, web_host_two: str) -> bool:
    return get_url_scheme(web_host_one) == get_url_scheme(web_host_two) and get_url_port(web_host_one) == get_url_port(
        web_host_two) and get_fqdn(web_host_one) == get_fqdn(web_host_two) and get_ipv4(web_host_one) == get_ipv4(
        web_host_two)


def are_web_hosts_equal_fast_version(web_host_one: Tuple[str, int, Optional[str], Optional[str]],
                                     web_host_two: Tuple[str, int, Optional[str], Optional[str]]) -> bool:
    return web_host_one[0] == web_host_two[0] and web_host_one[1] == web_host_two[1] and web_host_one[2] == \
        web_host_two[2] and web_host_one[3] == web_host_two[3]


def has_js_extension(url: str) -> bool:
    return get_url_extension(url) == 'js'
